---
title: "Introduction To Machine Learning"
output: html_document
---
#### This is the code by Dr Shirin Glander for Introduction to Machine Learning #####
### Link: https://www.r-bloggers.com/code-for-workshop-introduction-to-machine-learning-with-r/ ######
```{r}
###########################
### Packages installed ####
###########################

######### Set Up ##########
library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(mice)      # mice package for Multivariate
library(RColorBrewer) # ggplot colour 
library(igraph)  # for correlation graphs 

##### Data Preparation #####
### Dataset using is the Breast Cancer Wisconsin Dataset ###
### link: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/ ####

### predictors: malignant or benign breast mass ###
### 12 characteristic features ###
### dyplyr, mutate: ensure bare_nuclei is numerical and name the classes variable ###
breastcancerdata <- read_delim("~/Documents/Learn R/breast-cancer-wisconsin.data.txt",delim = ",",col_names = c("sample_ID", "clump_thickness","uniformity_ofcell_size","uniformity_of_cell_shape","marginal_adhesion","single_epithelial_cell_size","bare_nuclei","bland_chromatin","normal_nucleoli","mitosis","classes")) %>%
  mutate(bare_nuclei=as.numeric(bare_nuclei), classes=ifelse(classes=="2","benign",ifelse(classes=="4","malignant",NA)))

summary(breastcancerdata)

### dealing with missing data ###
# check NAs in the data #
md.pattern(breastcancerdata,plot = FALSE) # 16 NAs
breastcancerdata <- breastcancerdata %>% 
  drop_na() %>%
  select(classes,everything(), -sample_ID) # dropped sample ID and NAs 
head(breastcancerdata)  
summary(breastcancerdata)

```

###################################
######## Data Exploration #########
###################################
```{r}
### response classes is a slight bit unbalanced ###
### however not keep it simple for this case ###
### Response for classification ###
ggplot(breastcancerdata,aes(x=classes,fill=classes))+geom_bar()

### Response for regression ###
display.brewer.all()
ggplot(breastcancerdata,aes(x=clump_thickness,fill=clump_thickness))+geom_histogram(bins = 10)

### Features ###
# check gather #
aa<-gather(breastcancerdata,x,y,clump_thickness:mitosis) # 6147 rows of each variable as x and the actual value as y: 683*9=6147
gather(breastcancerdata,x,y,clump_thickness:mitosis) %>% ggplot(aes(x=y,colour=classes,fill=classes))+geom_density(alpha=0.3)+facet_wrap(~x,scales="free",ncol = 3)

# correlation graphs #
benignmatrix <- filter(breastcancerdata,classes=="benign") %>%
  select(-1) %>%
  cor()

malignantmatrix <- filter(breastcancerdata,classes=="malignant") %>%
  select(-1) %>%
  cor()

### link for igraph: http://kateto.net/networks-r-igraph ###
graph_benign <- graph.adjacency(benignmatrix,weighted = TRUE,diag = FALSE,mode = "upper")
graph_malignant <- graph.adjacency(malignantmatrix,weighted = TRUE,diag = FALSE,mode = "upper")

cut.off_benign <- mean(E(graph_benign)$weight)
cut.off_malignant <- mean(E(graph_malignant)$weight)

graph_benign_2 <- delete_edges(graph_benign,E(graph_benign)[weight<cut.off_benign])
graph_malignant_2<-delete_edges(graph_malignant,E(graph_malignant)[weight<cut.off_malignant])

cluster_benign<-cluster_fast_greedy(graph_benign_2)
cluster_malignant<-cluster_fast_greedy(graph_malignant_2)

par(mfrow=c(1,2)) # 1 row 2 cols display of graphs

plot(cluster_benign,graph_benign_2,vertex.size=colSums(benignmatrix)*10,vertex.frame.color=NA,vertex.label.color = "black",vertex.label.cex = 0.8,edge.width = E(graph_benign_2)$weight * 15,layout = layout_with_fr(graph_benign_2),ain = "Benign tumors")

plot(cluster_malignant, graph_malignant_2,vertex.size = colSums(malignantmatrix) * 10,vertex.frame.color = NA,  vertex.label.color = "black", vertex.label.cex = 0.8,edge.width = E(graph_malignant_2)$weight * 15,layout = layout_with_fr(graph_malignant_2),main = "Malignant tumors")


```


##### Principal Component Analysis #####
```{r }

library(ellipse)

# perform pca and extract scores 
pca1<-prcomp(as.matrix(breastcancerdata[,-1]),scale=TRUE,center = TRUE)
pca2<- as.data.frame(pca1$x)

# define groups for plotting
pca2$groups <- breastcancerdata$classes
centroids <- aggregate(cbind(PC1,PC2)~groups,pca2,mean)

conf.rgn<-do.call(rbind,lapply(unique(pca2$groups), function(t) data.frame(groups=as.character(t),ellipse(cov(pca2[pca2$groups==t,1:2]),
                                                                                                          centre = as.matrix(centroids[centroids$groups==t,2:3]),
                                                                                                          level=0.95),stringsAsFactors = FALSE)))

ggplot(data = pca2,aes(x=PC1,y=PC2,group=groups,color=groups))+
  geom_polygon(data = conf.rgn,aes(fill=groups),
               alpha=0.2)+geom_point(size=2,alpha=0.6)+
  labs(color="",fill="")

#### Multidimensional Scaling ####
select(breastcancerdata,-1) %>%
  dist() %>%
  cmdscale() %>% #### differ
  as.data.frame() %>%
  mutate(group=breastcancerdata$classes) %>%
  ggplot(aes(x=V1,y=V2,color=group)) +geom_point()

#### t-SNE dimensionality reduction ####
library(tsne)
## take some time to run 
# select(breastcancerdata, -1) %>%
#  dist() %>%
#  tsne() %>%  #### differ
#  as.data.frame() %>%
#  mutate(group = breastcancerdata$classes) %>%
#  ggplot(aes(x = V1, y = V2, color = group)) +geom_point()

```

##### Machine learning packages for R ####
#### Regression #####
```{r}
### Configure multicore
library(doParallel)
library(caret)

cluster <- makeCluster(detectCores())
registerDoParallel(cluster)

### train, validate and test data ###
set.seed(3)
index <- createDataPartition(breastcancerdata$classes,p=0.7,list = FALSE)

traindata<- breastcancerdata[index,] # 70% of breastdata
testdata <- breastcancerdata[-index,] # 30% of breastdata

bind_rows(data.frame(group = "train", traindata),
      data.frame(group = "test", testdata)) %>%
  gather(x, y, clump_thickness:mitosis) %>%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)

#### Regression ####
set.seed(3)
glm1 <- caret::train(clump_thickness ~., data=traindata,method="glm",preProces=c("scale","center"),trControl=trainControl(method = "repeatedcv",number=10,repeats = 10,savePredictions = TRUE,verboseIter = FALSE))
glm1

pred <- predict(glm1,testdata)

data.frame(residuals = resid(glm1),predictors =glm1$finalModel$linear.predictors) %>%
  ggplot(aes(x = predictors, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = "lm") 

data.frame(residuals = resid(glm1),y = glm1$finalModel$y) %>%
  ggplot(aes(x = y, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = "lm")

data.frame(actual = testdata$clump_thickness,
           predicted = pred) %>%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = "lm")

```
###### Classification #####
```{r}
#####################
### Decision Tree ###
#####################

library(rpart)
library(rpart.plot)

set.seed(3)
fit1 <- rpart(classes ~ .,
            data = traindata,
            method = "class",
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = "information"))
rpart.plot(fit1, extra = 100)

######################
### Random Forests ###
######################
### Random Forests predictions are based on the generation of multiple classification trees. They can be used for both, classification and regression tasks.
### Link:https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
set.seed(3)
library(e1071)
randomforest1 <- caret::train(classes ~ .,data = traindata,method = "rf",preProcess =c("scale","center"),trControl = trainControl(method = "repeatedcv",number = 5, repeats = 3,savePredictions = TRUE,verboseIter = FALSE))

randomforest1  # mtry=2 for final model
randomforest1$finalModel$confusion

### dealing with unbalanced data ###
### down for undersampling ###
set.seed(3)
randomforest2 <- caret::train(classes ~ .,data = traindata,method = "rf",preProcess =c("scale","center"),trControl = trainControl(method = "repeatedcv",number = 10, repeats = 10,savePredictions = TRUE,verboseIter = FALSE, sampling = "down"))

randomforest2

# Feature importance #
imp <- randomforest1$finalModel$importance
imp[order(imp,decreasing = TRUE),]
#estimate variable importance
importance <- varImp(randomforest1,scale=TRUE)
plot(importance)
# predict test data
confusionMatrix(predict(randomforest1,testdata),as.factor(testdata$classes))
results <- data.frame(actual = testdata$classes,predict(randomforest1, testdata, type = "prob"))

results$prediction <- ifelse(results$benign > 0.5, "benign",ifelse(results$malignant > 0.5, "malignant", NA))
results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)
ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")

ggplot(results,aes(x=prediction,y=benign,color=correct,shape=correct))+geom_jitter(size=3,alpha=0.6)

########################################
####Extreme Gradient Boosting Trees#####
########################################
### link: https://xgboost.readthedocs.io/en/latest/model.html###
set.seed(3)
extremegb <- caret::train(classes ~ .,data = traindata,
                          method = "xgbTree",preProcess =c("scale","center"),trControl = trainControl(method = "repeatedcv", 
number = 5, repeats = 3, savePredictions = TRUE,verboseIter = FALSE))
                                           
extremegb

results <- data.frame(actual = testdata$classes,
                      predict(extremegb, testdata, type= "prob"))
results$prediction <- ifelse(results$benign > 0.5, "benign",ifelse(results$malignant > 0.5, "malignant", NA))
results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)
ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")
ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)

### useful links ###
### available models in caret ###
### link: https://topepo.github.io/caret/available-models.html ###

```
################################
###### Feature Selection #######
################################
```{r}
library(corrplot)

## calculate correlation matrix
cormatrix <- cor(traindata[,-1])
corrplot(cormatrix,order="hclust")

## apply correlation filter at 0.7
highlycor <- colnames(traindata[,-1])[findCorrelation(cormatrix,cutoff=0.7,verbose = TRUE)]

## variables that are flagged for removal
highlycor

## remove the two variables
traindata_new<- traindata[,which(!colnames(traindata)%in%highlycor)]

#### recursive feature elimination ####
#### genetic algorithm ####


```

####### Hyperparameter tuning with caret #######
################################################
```{r}
set.seed(3)
grid <- expand.grid(mtry=c(1:10))
randomforest3_tuning <- caret::train(classes ~ .,
                         data = traindata,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
number = 10, repeats = 10, savePredictions = TRUE,verboseIter = FALSE),tuneGrid = grid)
randomforest3_tuning
plot(randomforest3_tuning)
```
